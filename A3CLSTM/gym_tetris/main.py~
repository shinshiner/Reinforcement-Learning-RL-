from __future__ import print_function, division
import os
os.environ["OMP_NUM_THREADS"] = "1"
import argparse
import sys
import torch
import torch.multiprocessing as mp
from torch.multiprocessing import Process, Lock, Value
from model import agentNET
from train import train
from test import test
from shared_optim import SharedRMSprop, SharedAdam
import time
from env import *

parser = argparse.ArgumentParser(description='A3C')
parser.add_argument(
    '--train',
    default=True,
    metavar='L',
    help='train model')
parser.add_argument(
    '--gpu',
    default=False,
    metavar='L',
    help='gpu model')
parser.add_argument(
    '--rwdload',
    default=False,
    metavar='L',
    help='load trained reward models')
parser.add_argument(
    '--agentload',
    default=False,
    metavar='L',
    help='load trained agent models')
parser.add_argument(
    '--lr',
    type=float,
    default=0.0001,
    metavar='LR',
    help='learning rate')
parser.add_argument(
    '--rwdlr',
    type=float,
    default=0.00005,
    metavar='confLR',
    help='reward network learning rate')
parser.add_argument(
    '--gamma',
    type=float,
    default=0.99,
    metavar='G',
    help='discount factor for rewards')
parser.add_argument(
    '--tau',
    type=float,
    default=1.00,
    metavar='T',
    help='parameter for GAE')
parser.add_argument(
    '--seed',
    type=int,
    default=233,
    metavar='S',
    help='random seed')
parser.add_argument(
    '--workers',
    type=int,
    default=0,
    metavar='W',
    help='how many training processes to use')
parser.add_argument(
    '--now',
    type=int,
    default=0,
    metavar='W',
    help='now stage')
parser.add_argument(
    '--step-map',
    type=int,
    default=5,
    metavar='W',
    help='the step length')
parser.add_argument(
    '--step-add',
    type=int,
    default=18,
    metavar='W',
    help='the num of step add')
parser.add_argument(
    '--num-steps',
    type=int,
    default=15,
    metavar='NS',
    help='number of forward steps in A3C')
parser.add_argument(
    '--limit-map',
    type=float,
    default=295,
    metavar='M',
    help='the rate length of an episode')
parser.add_argument(
    '--max-steps',
    type=int,
    default=1500,
    metavar='NS',
    help='number of max steps')
parser.add_argument(
    '--env',
    default='map',
    metavar='ENV',
    help='environment to train on')
parser.add_argument(
    '--load-model-dir',
    default='trained_models/',
    metavar='LMD',
    help='folder to load trained models from')
parser.add_argument(
    '--save-model-dir',
    default='trained_models/',
    metavar='SMD',
    help='folder to save trained models')
parser.add_argument(
    '--log-dir', 
    default='logs/', 
    metavar='LG', 
    help='folder to save logs')
parser.add_argument(
    '--div', 
    type = int,
    default=20, 
    metavar='NS', 
    help='the pieces the path is divided')

if __name__ == '__main__':
    args = parser.parse_args()
    mp.set_start_method('spawn')
    torch.set_default_tensor_type('torch.FloatTensor')
    torch.manual_seed(args.seed)

    if args.train:
        shared_model = agentNET()
        if args.agentload:
           saved_state = torch.load('{0}{1}_agent.dat'.format(args.load_model_dir, args.env))
           shared_model.load_state_dict(saved_state)
        shared_model.share_memory()
        optimizer = SharedAdam(shared_model.parameters(), lr=args.lr)
        optimizer.share_memory()

        while True:
            processes = []
            p = Process(target=test, args=(args, shared_model))
            p.start()
            processes.append(p)
            time.sleep(0.1)
            for rank in range(0, args.workers):
                p = Process(target=train, args=(rank, args, shared_model, optimizer))
                p.start()
                processes.append(p)
                time.sleep(0.1)
            for rank in range(0, args.workers):
                processes[rank].join()
    else:
        evaluation(args)
